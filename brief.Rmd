---
title: TV-SOBI Algorithm with R
bibliography: ref.bibtex
nocite: | 
  @hyvarinen2000independent,
  @miettinen2017blind,
  @hyvarinen2013independent,
  @baloch2005robust,
  @miettinen2016separation,
  @virta2017blind,
  @yeredor2003tv 
---

Last updated 29 December 2018

Time-varying second order blind source separation.

_R implementation code has been removed from this page, please refer to dedicated code page at [Github](https://github.com/yyyaaan/bss)_

## Concept of BSS

$$\begin{aligned} 
\boldsymbol x_t =  \boldsymbol \Omega_t \boldsymbol  z_t
\\ & \begin{cases}  
\text{zero-mean} & E( \boldsymbol z_t) = \boldsymbol 0 ,\ \ \forall t \text{ pre-centered}
\\ \text{independence} & \text{components of } \boldsymbol z \text{ are uncorrelated}
\\ \text{stationary} & \text{the process } ( \boldsymbol z_t)_{t\in \mathbb Z} \text{ weak stationary time-series}
\\ \text{time-varying mixing}& \boldsymbol \Omega_t =  ( \boldsymbol I + t \boldsymbol{\mathcal E})\boldsymbol \Omega_0 
\\ \text{slow time-variation} & \boldsymbol{\mathcal E} << \boldsymbol I
\end{cases} 
\end{aligned}$$

## TV-SOBI Problem Formulation

$$\begin{aligned} 
\boldsymbol x_t =  ( \boldsymbol I + t \boldsymbol{\mathcal E})\boldsymbol \Omega_0 \boldsymbol  z_t
\\
\\ (1.1)\ & E( \boldsymbol z_t) = \boldsymbol 0 
\\ (1.2)\ & E( \boldsymbol z_t \boldsymbol z_t') = \text{Cov}( \boldsymbol z_t) = \boldsymbol I
\\ (2.1)\ & E( \boldsymbol z_t \boldsymbol z_{t+\tau}') = \boldsymbol\Lambda_\tau \text{ diagonal for all } \tau = 1,2,\dots
\\
\\ \text{Given } \boldsymbol x_t, \text{ optimize } \boldsymbol\Omega_0 \text{ and } \boldsymbol{\mathcal E}  &\text{ for (2.1) under restricstions of (1.1) and (1.2)}
\end{aligned}$$

<blockquote>
Two types of 3-dimensional signal mixture: ordinary mixing (left) and time-varying mixing (right). The source signals are the same. $t=1,2,\dots, 1000,\ \ \boldsymbol \epsilon\approx 10^{-4}\boldsymbol{I}$
</blockquote>

![ ](zzzMixing.png)

## Autocovariances

$$\begin{aligned}
\text{Cov}( \boldsymbol x_t , \boldsymbol x_{t+\tau}) &= E(\boldsymbol x_t \boldsymbol x_{t+\tau}')
\\ & = E[( \boldsymbol I + t \boldsymbol{\mathcal E})\boldsymbol \Omega_0 \boldsymbol  z_t\ \boldsymbol  z_{t+\tau} ' \boldsymbol \Omega_0 '( \boldsymbol I + t \boldsymbol{\mathcal E}')]
\\ &= \underbrace{\boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0'}_{:= \boldsymbol{Ra}_\tau}
 + t (\underbrace{ \boldsymbol{\mathcal E } \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' +  \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'} _{:= \boldsymbol{Rb}_\tau} )
 + t^2 (\underbrace{\boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'}_{:= \boldsymbol{Rc}_\tau})
 + \underbrace {\tau (\boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0')
 + t \tau (\boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}')}_\text{ignorable}
\\ &\approx \boldsymbol{Ra}_\tau + t\, \boldsymbol{Rb}_\tau + t^2\, \boldsymbol{Rc}_\tau
\end{aligned}$$

Then, the solutions for $\boldsymbol\Omega_0$ and $\boldsymbol{\mathcal E}$ can be found through the equation group

$$\begin{cases}
E(\boldsymbol x_t \boldsymbol x_{t+\tau}') =  \boldsymbol{Ra}_\tau + t\, \boldsymbol{Rb}_\tau + t^2\, \boldsymbol{Rc}_\tau  & \text{(1) linear model}\\
\boldsymbol{Ra}_\tau = \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0'  & \text{(2a) optimization} \\
\boldsymbol{Rb}_\tau = \boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' +  \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}' & \text{(2b) optimization}\\ 
\boldsymbol{Rc}_\tau = \boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'  & \text{(2c) optimization} \end{cases}$$

<blockquote>
Autocovariance matrices at different lags can use `stats::acf` function (`stats` are pre-installed and pre-loaded packages in `R`). Considering the algorithm, __it is not necessary to calculate the autocovariance matricies__ at all.
</blockquote>

## Algorithm

### Step 1 Linear Regression

Estimate $\boldsymbol {Ra}_\tau, \ \boldsymbol {Rb}_\tau, \ \boldsymbol {Rc}_\tau$ through a linear model (__element-wise__) with the empirical autocovariance.

$$\begin{aligned} 
\text{for each desired lags } \tau = 1,2,\dots, L &
\\ \text{for each } i = 1,2,\dots, & p
\\ \text{for each } j = & 1,2,\dots, p
\\ 
\\ \underbrace {\begin{bmatrix} E(\boldsymbol x_{1} \boldsymbol x'_{1+\tau})[i,j] \\ E(\boldsymbol x_{2} \boldsymbol x'_{2+\tau})[i,j] \\ E(\boldsymbol x_{3} \boldsymbol x'_{3+\tau})[i,j]  \\ \vdots \\ E(\boldsymbol x_{n-\tau} \boldsymbol x'_{n})[i,j] \end{bmatrix}}_{ \boldsymbol y_\tau [i,j]}
 &= \underbrace{ \begin{bmatrix}  1 & 1 & 1^2 \\ 1 & 2 & 2^2\\  1 & 3 & 3^2\\ \vdots & \vdots & \vdots \\  1 & (n-\tau) & (n-\tau)^2 \end{bmatrix}  }_{ \boldsymbol H} \ 
 \underbrace{ \begin{bmatrix} \boldsymbol {Ra}[i,j] \\ \boldsymbol{Rb}[i,j] \\ \boldsymbol {Rc} [i,j] \end{bmatrix}  }_{ \boldsymbol \theta_\tau [i,j]} 
\\ \\ \text{LS/ML-estimator gives }
\\ \begin{bmatrix} \boldsymbol {Ra}[i,j] \\ \boldsymbol {Rb}[i,j] \\ \boldsymbol {Rc}[i,j] \end{bmatrix} &= ( \boldsymbol H ' \boldsymbol H)^{-1} \boldsymbol H' \boldsymbol y_\tau[i,j]
\end{aligned}$$

The definition of $\boldsymbol{Ra},\boldsymbol{Rb},\boldsymbol{Rc}$ implies the symmetric structure, while the linear estimator does not. Thus, a simple trick is applied as $\boldsymbol{Ra} \leftarrow(\boldsymbol{Ra} + \boldsymbol{Ra}')/2$.

### Step 2 Sub-SOBI procedure for $\boldsymbol W_0 = \boldsymbol {C\Omega_0} ^{-1}$

$\boldsymbol {Ra}$ can be viewed as the autocovariance of another p-variate time series, and the estimation follows exactly the SOBI.Suppose the unmixing matrix $\boldsymbol W_0 = \boldsymbol\Omega_0 ^{-1}$.

__ISSUE__ Unlike (auto)covariance matrix, $\boldsymbol{Ra}$ may NOT be positive semi-definite, which makes whitening impossible.

$$\begin{aligned}
\text{Whitening } \tilde {\boldsymbol{Ra}}_\tau &= \boldsymbol{Ra}_0^{-1/2}\ \boldsymbol {Ra} _\tau\ \boldsymbol {Ra}_0^{-1/2\ '}
\\
\\ \text{JD for orthognal } \boldsymbol V &\begin{cases} 
\boldsymbol V \tilde {\boldsymbol R}_1^{(0)} \boldsymbol V' & =  \boldsymbol\Lambda_1 \\
\boldsymbol V \tilde {\boldsymbol R}_2^{(0)} \boldsymbol V' & =  \boldsymbol\Lambda_2 \\
&\vdots \\
\boldsymbol V \tilde {\boldsymbol R}_3^{(0)} \boldsymbol V' & =  \boldsymbol\Lambda_3 \\
\end{cases}
\\
\\ \text{Finally, } \boldsymbol W_0 &= \boldsymbol V \boldsymbol Ra_0^{-1/2}
\\ \Leftrightarrow \boldsymbol\Omega_0 &= \boldsymbol Ra_0^{1/2} \boldsymbol V
\end{aligned}$$

<blockquote>
A function is created to combine the above procedures, i.e. first whitening then joint-diagnolizing (this may be useful later). This function solves for square matrix (not orthogonal) that makes $\Lambda_i = W'A_i W$ as diagonal as possible.
</blockquote>

### Step 3 Method 1&2 Estimate $\boldsymbol{\mathcal E}$ via $\boldsymbol{Rb}$

This step can be transformed to a linear optimization problem after designing proper matricies based on pervious results; the ML/LS-estimator give the estimator as in Step 1. Method 1 and 2 are very similar, and the only difference is the choice of $\boldsymbol Q$. Method 2 use the results from joint diagonalization in the previous steps.

$$\begin{aligned}
\text{sum for } \tau = 1,2,\dots,L
\\ \text{vec}( \boldsymbol {Rb}_\tau) &= \big( \underbrace{\boldsymbol H_{\tau,1} + \boldsymbol H_{\tau,2} }_{ \boldsymbol H} \big) \text{vec}( \boldsymbol{\mathcal E})  
\\ \text{where } &   \boldsymbol H \text{ are carefully designed transformation based on } \boldsymbol Q
\\ & \begin{cases}
\text{Method 1} & \boldsymbol Q= \boldsymbol {Ra}_\tau 
\\  \text{Method 2} & \boldsymbol Q=  \hat{\boldsymbol\Omega_0}\ \hat{ \boldsymbol\Lambda_\tau} \ \hat{\boldsymbol\Omega_0}'
\end{cases}
\end{aligned}$$

<blockquote>
In R implementation, a handy function is written for constructing the design matrix $\boldsymbol H$ because the design is the same with 2 methods. Attention is needed for vectorization by row or by column ($y.design$ by column, $\boldsymbol{\mathcal E}$ by row).

The accuracy measurement is not clear (see "issues" page)
</blockquote>


### Step 3 Method 2 Estimate $\boldsymbol{\mathcal E}$ via $\boldsymbol{Rc}$

The third approach may potentially be less accurate due to the use of (2c), which are supposed to carry less information. It is then a joint diagonalization problem searching for $\boldsymbol{\mathcal E \Omega_0}$. Note that it faced challenge as in step 2 with regards to the postive definite property.

$$\boldsymbol R_\tau^{(2)} = \boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'$$

<blockquote>
This joint diagnolization is exactly the same as step 2, and the only extra task is to recover $\boldsymbol{\mathcal E}$ from $\boldsymbol{\mathcal E\Omega}_0$ using $\hat{ \boldsymbol\Omega}_0$.
</blockquote>

### Step 3 Method 3 Analytically Solve for $\boldsymbol{\mathcal E}$

Furthermore, observe that it can be solve analytically (in theory) by the formula given in (2b) after element-wise accumulation over $\tau=1,2,\dots,L$. The result should be the same as method 1.

$$ \sum\limits_{\tau=1}^L \boldsymbol {Rb}_\tau = \boldsymbol{\mathcal E} \bigg( \sum\limits_{\tau=1}^L \boldsymbol{Ra}_\tau \bigg) + \bigg( \sum\limits_{\tau=1}^L \boldsymbol{Ra} _\tau'\bigg) \boldsymbol{\mathcal E'}$$

##R-Notes

The structure of data matrix is in fact transposed ($n\times p$ in `R`). 
<br />
Vectorization and matrix creation are by column with default settings.

## References

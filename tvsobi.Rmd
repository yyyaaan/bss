---
title: TV-SOBI
---

## Concept

$$\begin{aligned} 
\boldsymbol x_t =  \boldsymbol \Omega_t \boldsymbol  z_t
\\ & \begin{cases}  
\text{zero-mean} & E( \boldsymbol z_t) = \boldsymbol 0 ,\ \ \forall t \text{ pre-centered}
\\ \text{independence} & \text{each component of } \boldsymbol z \text{ is mutually independent}
\\ \text{stationary} & \text{the process } ( \boldsymbol z_t)_{t\in \mathbb Z} \text{ weak stationary time-series} 
\end{cases}
\\
\\ & \begin{cases}  
t = \dots, -2,-1,0,1,2,\dots \text{ centered }
\\ \text{time-varying mixing matrix } \boldsymbol \Omega_t =  ( \boldsymbol I + t \boldsymbol{\mathcal E})\boldsymbol \Omega_0 
\\ \text{slow variation of mixing } \boldsymbol{\mathcal E} << \boldsymbol I
\end{cases} 
\end{aligned}$$

## Formulation

$$\begin{aligned} 
\boldsymbol x_t =  ( \boldsymbol I + t \boldsymbol{\mathcal E})\boldsymbol \Omega_0 \boldsymbol  z_t
\\
\\ (1.1)\ & E( \boldsymbol z_t) = \boldsymbol 0 
\\ (1.2)\ & E( \boldsymbol z_t \boldsymbol z_t') = \text{Var}( \boldsymbol z_t) = \boldsymbol I
\\ (2.1)\ & E( \boldsymbol z_t \boldsymbol z_{t+\tau}') = \boldsymbol\Lambda \text{ diagonal and constant }\forall \tau = 1,2,\dots
\\
\\ \text{Given } \boldsymbol x_t, \text{ optimize } \boldsymbol\Omega_0 \text{ and } \boldsymbol{\mathcal E}  &\text{ for (2.1) under restricstions of (1.1) and (1.2)}
\end{aligned}$$

## Autocovariance (Second-Order)

$$\begin{aligned} 
\text{Let unmixing matrix } & \boldsymbol \Gamma = \boldsymbol {C \Omega_0}^{-1}, \text{ where } \boldsymbol C \text{ permutation matrix} 
\\ \text{Let "mixed autocovariance"}& \boldsymbol R_\tau^{(0)} = \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \text{ for convenience (symmetric)}
\\
\\ \text{Cov}( \boldsymbol x_t , \boldsymbol x_{t+\tau}) &= E(\boldsymbol x_t \boldsymbol x_{t+\tau}')
\\ & = E[( \boldsymbol I + t \boldsymbol{\mathcal E})\boldsymbol \Omega_0 \boldsymbol  z_t\ \boldsymbol  z_{t+\tau} ' \boldsymbol \Omega_0 '( \boldsymbol I + t \boldsymbol{\mathcal E}')]
\\ &= \boldsymbol R_\tau^{(0)}
 + t (\boldsymbol{\mathcal E R}^{(0)}_\tau +  \boldsymbol{R}^{(0)}_\tau \boldsymbol{\mathcal E}')
 + t^2 (\boldsymbol{\mathcal E R}^{(0)}_\tau \boldsymbol{\mathcal E}')
 + \underbrace {\tau (\boldsymbol{\mathcal E R}^{(0)}_\tau)
 + t \tau (\boldsymbol{\mathcal E R}^{(0)}_\tau \boldsymbol{\mathcal E}')}_\text{ignorable}
\\ &\approx \boldsymbol R_\tau^{(0)} + t \boldsymbol R_\tau^{(1)} + t^2 \boldsymbol R_\tau^{(2)}
\end{aligned}$$

Then, the solutions for $\boldsymbol\Omega_0$ and $\boldsymbol{\mathcal E}$ can be found through the equation group

$$\begin{cases} 
E(\boldsymbol x_t \boldsymbol x_{t+\tau}') = \boldsymbol R_\tau^{(0)} + t \boldsymbol R_\tau^{(1)} + t^2 \boldsymbol R_\tau^{(2)}  & \text{(1) linear model}\\
\boldsymbol R_\tau^{(0)} = \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0'  & \text{(2a) optimization} \\
\boldsymbol R_\tau^{(1)} = \boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' +  \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}' & \text{(2b) optimization}\\ 
\boldsymbol R_\tau^{(2)} = \boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'  & \text{(2c) optimization} \end{cases}$$

## Algorithm

### Step 1 Linear Regression

Estimate $\boldsymbol R_\tau^{(0)}, \ \boldsymbol R_\tau^{(1)}, \ \boldsymbol R_\tau^{(2)}$ through a linear model (element-wise) with the empirical autocovariance.

$$\begin{aligned} 
\text{for each desired lags } \tau = 1,2,\dots, L &
\\ \text{for each } i = 1,2,\dots, & p
\\ \text{for each } j = & 1,2,\dots, p
\\ 
\\ \underbrace {\begin{bmatrix} \vdots \\ E(\boldsymbol x_{-1} \boldsymbol x'_{-1+\tau})[i,j] \\ E(\boldsymbol x_{0} \boldsymbol x'_{\tau})\ [i,j] \\ E(\boldsymbol x_{1} \boldsymbol x'_{1+\tau})[i,j] \\ E(\boldsymbol x_{2} \boldsymbol x'_{2+\tau})[i,j] \\ \vdots  \end{bmatrix}}_{ \boldsymbol y_\tau [i,j]}
 &= \underbrace{ \begin{bmatrix} \vdots &\vdots &\vdots \\ 1 & -1 & (-1)^2 \\ 1 & 0 & 0^2 \\ 1 & 1 & 1^2 \\ 2 & 2 & 2^2\\ \vdots & \vdots & \vdots \end{bmatrix}  }_{ \boldsymbol H} \ 
 \underbrace{ \begin{bmatrix} \boldsymbol R_\tau^{(0)}[i,j] \\ \boldsymbol R_\tau^{(1)}[i,j] \\ \boldsymbol R_\tau^{(2)}[i,j] \end{bmatrix}  }_{ \boldsymbol \theta_\tau [i,j]} 
\\ \\ \text{LS/ML-estimator gives }
\\ \begin{bmatrix} \boldsymbol R_\tau^{(0)}[i,j] \\ \boldsymbol R_\tau^{(1)}[i,j] \\ \boldsymbol R_\tau^{(2)}[i,j] \end{bmatrix} &= ( \boldsymbol H ' \boldsymbol H)^{-1} \boldsymbol H' \boldsymbol y_\tau[i,j]
\end{aligned}$$

### Step 2 Sub-SOBI procedure for $\boldsymbol\Omega_0$

$\boldsymbol R_\tau^{(0)}$ can be viewed as the autocovariance of another p-variate time series, and the estimation follows exactly the SOBI.

$$\begin{aligned}
\text{Spatial-whitening } \tilde {\boldsymbol R}_\tau^{(0)} &= \bigg(\boldsymbol R_0^{(0)}\bigg)^{-1/2} \boldsymbol R_\tau^{(0)} \bigg(\boldsymbol R_0^{(0)}\bigg)^{-1/2\ '}
\\
\\ \text{rjd or djd for orthognal } \boldsymbol U &\begin{cases} 
\boldsymbol U \tilde {\boldsymbol R}_1^{(0)} \boldsymbol U' & =  \boldsymbol\Lambda_1 \\
\boldsymbol U \tilde {\boldsymbol R}_2^{(0)} \boldsymbol U' & =  \boldsymbol\Lambda_2 \\
&\vdots \\
\boldsymbol U \tilde {\boldsymbol R}_3^{(0)} \boldsymbol U' & =  \boldsymbol\Lambda_3 \\
\end{cases}
\\
\\ \boldsymbol\Omega_0 &= \bigg(\boldsymbol R_0^{(0)}\bigg)^{1/2} \boldsymbol U
\end{aligned}$$

### Step 3 Estimate $\boldsymbol{\mathcal E}$

Potentially 3 methods

First, it can be transformed to a linear optimization problem after vectorization, and the ML/LS-estimator give the estimator as in Step 1.

$$\begin{aligned}
\tau = 1,2,\dots,L
\\ \text{vec}( \boldsymbol R _\tau^{(1)}) &= \big( \boldsymbol H_{\tau,1} + \boldsymbol H_{\tau,2}  \big) \text{vec}( \boldsymbol{\mathcal E})  
\\ \text{where } &   \boldsymbol H_{\tau,1}, \ \boldsymbol H_{\tau,2} \text{ are proper vectorization of }  \boldsymbol R_{\tau}^{(0)}
\end{aligned}$$

Second, it can be solve analytically by the formula given in (2b) after sum-up $\tau=1,2,\dots,L$

$$ \sum\limits_{\tau=1}^L \boldsymbol R_\tau^{(1)} = \boldsymbol{\mathcal E} \bigg( \sum\limits_{\tau=1}^L \boldsymbol R_\tau^{(0)} \bigg) + \bigg( \sum\limits_{\tau=1}^L \boldsymbol R_\tau^{(0)'} \bigg) \boldsymbol{\mathcal E'}   $$

The third appraoch may potentially be less accurate due to the use of (2c), which are supposed to carry less information. It is a joint diagonalization problem searching for $\boldsymbol{\mathcal E \Omega_0}$, but the challenge could be spatial whitening.

$$\boldsymbol R_\tau^{(2)} = \boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'$$

The methods 1 and 2 obviously coincide each other in the unique solution. The 3rd method ignores $\boldsymbol R_\tau^{(1)}$
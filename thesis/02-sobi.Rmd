# Blind Source Separation {#sobi}

Blind Source Separation (BSS) assumes that the observed signal vector $\boldsymbol{x}$ is a static mixture of $p$-dimensional latent source vector $\boldsymbol{z}$, and the ordinary blind source model can be written as $\boldsymbol{x} = \boldsymbol{\mu} + \boldsymbol{\Omega z}'$, where $\boldsymbol{\Omega}$ denotes $p\times p$ mixing matrix, and $\boldsymbol{\mu}$ stands for the location, commonly mean vector, of $\boldsymbol{x}$ [@belouchrani1997blind]. This model can be easily expanded to stochastic process case by using $p$-vectors, i.e. $p\times T$ matrices, $\boldsymbol{x}=(\boldsymbol{x}_1, \boldsymbol{x}_2,\dots,\boldsymbol{x}_p)'$ and $\boldsymbol{z}=(\boldsymbol{z}_1, \boldsymbol{z}_2,\dots,\boldsymbol{z}_p)'$. Without loss of generality, it can be assumed that the observed signals has zero-mean vector, as the observation can be pre-processed by $\boldsymbol x = \boldsymbol x^{\text{obs}} - \boldsymbol \mu$. Thereafter, the model can be further simplified into the form of $\boldsymbol{x} = \boldsymbol{\Omega z}$. This thesis specifies the observations and signals matrices to be $p\times T$ organized in the way that each series of signals is presented as row-vector, which is consistent with major literature in relevant domain, but is usually a transposed form in data science programming including R Data Frame and Python Pandas. Finally, the basic multidimensional form of blind source separation is,

\begin{equation}
\underset{(p\times n)}{\boldsymbol x} = \underset{(p\times p)}{\boldsymbol{\Omega}} \ \underset{(p\times n)}{\boldsymbol{z}}, \ \ \text{ where } \boldsymbol{x} \text{ is the only observed zero-mean }p \text{-vector}
(\#eq:SOM)
\end{equation}

Although mathematically unnecessary, it is assumed that $\boldsymbol{\Omega}$ is a full-rank $p\times p$ matrix. In fact, a decrease in dimension of $\boldsymbol{\Omega}$ will allow fewer signal series (number of rows in $\boldsymbol z$) generated from the same observation $\boldsymbol{x}$. However, it is not necessary due to the fact that the output carries real-world information, and the usefulness of series can be better determined using evidence of the signal itself. Consequently, it is a justified choice to force the mixing matrix to be full-rank.

The goal of BSS is always to restore the source signals based on observation, and the above model assures that finding mixing matrix $\boldsymbol{\Omega}$ or unmixing matrix $\boldsymbol{W}= \boldsymbol{\Omega}^{-1}$ suffices, where the model assumption of full rank guarantees the existence of inverse matrix. For notation simplification and clarity, the following paragraphs shall only use $\boldsymbol\Omega$. 

## Ambiguities and Assumptions

The BSS equation \@ref(eq:SOM) underlies the impossibility to solve $\boldsymbol\Omega$ and $\boldsymbol z$ in a finite closed form because of merely one known item, and thus BSS solution always incurs [@belouchrani1997blind],

- Permutation ambiguity: the $a$-th series of signal may be confused into $b$-th series in the restored one; however, they are always 1-1 mapping after sign-correction. The permutation can also include sign ambiguity.
- Scale ambiguity: restore signals can be scaled due to the fact that for any (unknown) scale constant $||a||:\ \boldsymbol x \equiv \big(\boldsymbol\Omega ||a||^{-1} \big) \big(||a|| \boldsymbol z \big)$.

Figure \@ref(fig:ambiguity) illustrates the ambiguity. The observed source is a 4-dimensional signal combining auto-regressive, moving average, sinusoidal and ECG-style time series. Comparing left and right plot, observation series 2 is clearly mapped into series 1 in restored signals, while series 4 become series 2. Although the shape and waveform are very similar, the scale of the y-axis intimates the scale ambiguity.

```{r ambiguity, fig.show='hold', fig.cap='Observed vs. restored sources', out.width='50%', echo = FALSE}
load("thesis.rdata")
plot.ts(fig_mixing$source, main = "Observed Source",      ann = FALSE)
plot.ts(fig_mixing$unmix,  main = "SOBI Restored Source", ann = FALSE)
```

There exist different approaches to solve the BSS problem in terms of identifying the mixing matrix and source signal series. Notably, the Independence Component Analysis (ICA) algorithm assumes statistical independence of between series and nongaussian distribution within series, in addition to the optional setting of component number equality between source and mixture [@comon1994independent].


## Stationary Time-Series Source Separation Using Autocovariance Matrices

In information processing, the signals are usually recorded by highly precise measurement instruments at a given time interval, and thus arose the natural time-series data structure. Assuming pre-centered data, the time series observation can be easily adapted to the basic form \@ref(eq:SOM) by introducing the temporal subscript of $t=1,2,\dots, T$ as $\boldsymbol{x}_t = \boldsymbol{\Omega z}_t$. Focusing on the autocovariance matrices, a second order statistics, the second order source separation (SOS) model seeks to extract the original source signals based mainly on uncorrelatedness Furthermore, the source signals are assumed to be weakly stationary, implying that the autocovariance matrices vary only on the lag $\tau$ but not on the time of observation $t$, which mathematically express as $\mathbb E (\boldsymbol z_t \boldsymbol z_{t+\tau}')$ are constant given $\tau$ for all $t=1,2,\dots,T$. The SOS model assumes the discrete stochastic process $(\boldsymbol Z_t)_{t=0,\pm1, \pm2}$ as source signals and the mixture $(\boldsymbol X_t)_{t=0,\pm1, \pm2}$ such that [adapted @miettinen2016separation],
$$
\begin{equation}
\begin{aligned} 
\boldsymbol X_t = \boldsymbol{\Omega}\boldsymbol Z'_t,\ \ &\ t=0,\pm 1,\pm2,\dots
\\ \text{satisfies }(1)\ & \mathbb E( \boldsymbol Z_t) = \boldsymbol 0 
\\ (2)\ & \mathbb E( \boldsymbol Z_t \boldsymbol Z_t') = \text{Cov}( \boldsymbol Z_t) = \boldsymbol I_p
\\ (3)\ & \mathbb E( \boldsymbol Z_t \boldsymbol Z_{t+\tau}') = \boldsymbol\Lambda_\tau \text{ diagonal for all } \tau = 1,2,\dots
\end{aligned}    
(\#eq:SOS)
\end{equation}
$$
Give realization of the stochastic progress, this semi-parametric model become $\boldsymbol x_t = \boldsymbol\Omega \boldsymbol z_t$ and can thus be solved by joint optimization for the diagonal properties in assumption 3 under the restriction of assumption 2 in \@ref(eq:SOS), which become a unique constrained optimization using Lagrange method after proper whitening and/or normalization procedures [@miettinen2016separation]. 

The Second Order Blind Identification (SOBI) solves the BSS problem using second-order statistics when the true signal follows SOS model, and commonly autovariance matrices and the robust corresponder are applied [@belouchrani1997blind; @nordhausen2014robustifying]. The major second-order blind source separation approaches include AMUSE (Algorithm for Multiple Unknown Signals Extraction) by Tong, Soon, Huang and Liu [-@tong1990amuse] and Second Order Blind Identification, commonly know as SOBI, that originally proposed by Belouchrani, Abed-Meraim, Cardoso and Moulines [-@belouchrani1997blind]. Nordhausen [-@nordhausen2014robustifying] expanded the algorithm to non-stationary time series using locally stationary intervals and further robustifying with average spatial-sign autocovariances on such intervals.



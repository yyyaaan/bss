# Second Order Blind Source Separation {#sobi}

Blind Source Separation (BSS) assumes that the observed signal vector $\boldsymbol{x}$ is a static mixture of $p$-variate latent source vector $\boldsymbol{z}$, and the basic blind source model can be written as $\boldsymbol{x} = \boldsymbol{\mu} + \boldsymbol{\Omega z}'$, where $\boldsymbol{\Omega}$ denotes $p\times p$ mixing matrix, and $\boldsymbol{\mu}$ stands for the location, commonly mean vector, of $\boldsymbol{x}$ [@belouchrani1997blind]. This model can be easily expanded to multiple observation case by using $p$-vectors, i.e. $p\times n$ matrices, $\boldsymbol{x}=(\boldsymbol{x}_1, \boldsymbol{x}_2,\dots,\boldsymbol{x}_p)'$ and $\boldsymbol{z}=(\boldsymbol{z}_1, \boldsymbol{z}_2,\dots,\boldsymbol{z}_p)'$. Without loss of generality, it can be assumed that the source signals has zero-mean vector, as the observations can be pre-processed by $\boldsymbol x = \boldsymbol x^{\text{obs}} - \boldsymbol \mu$. Thereafter, the model can be further simplified into the form of $\boldsymbol{x} = \boldsymbol{\Omega z}$. This thesis specifies the observations and signals matrices to be $p\times T$ organized in the way that each dimension of time-series signals is presented as row-vector, which is consistent with major literature in relevant domain, but is usually transposed from data science programming R and Python Pandas. Finally, the basic time series form of blind source separation is,

\begin{equation}
\underset{(p\times n)}{\boldsymbol x} = \underset{(p\times p)}{\boldsymbol{\Omega}} \ \underset{(p\times n)}{\boldsymbol{z}}, \ \ \text{ where } \boldsymbol{x} \text{ is the only observed zero-mean }p \text{-vector}
(\#eq:SOM)
\end{equation}

Although mathematically unnecessary, it is assumed that $\boldsymbol{\Omega}$ is a full-rank $p\times p$ matrix. In fact, an decrease in dimension of $\boldsymbol{\Omega}$ will allow fewer signal components (number of columns in $\boldsymbol z$) generated from the same observation $\boldsymbol{x}$. However, it is not necessary due to fact that the output carries real-world information, and the usefulness of signal components (columns) can be better determined using scientific evidence of the signal itself. Consequently, it is a justified choice to force the mixing matrix to be full-rank.

The goal of BSS is always to restore the independent source signals, and the above model assures that finding mixing matrix $\boldsymbol{\Omega}$ or unmixing matrix $\boldsymbol{W}= \boldsymbol{\Omega}^{-1}$ suffices, where the model assumption of full rank guarantees the existence of inverse matrix. For notation simplification and clarity, the following paragraphs shall only use $\boldsymbol\Omega$. The Second order BSS solves the BSS problem using second order statistics, notably measures of scale including variances, median absolute deviation and other 2nd moment estimators [@belouchrani1997blind; @nordhausen2014robustifying]. The major second order blind source separation approaches include AMUSE (Algorithm for Multiple Unknown Signals Extraction) by Tong, Soon, Huang and Liu [-@tong1990amuse] and Second Order Blind Identification, commonly know as SOBI, that originally proposed by Belouchrani, Abed-Meraim, Cardoso and Moulines [-@belouchrani1997blind].

## Ambiguities of SOBI

Consider the BSS equation \@ref(eq:SOM), it is impossible to solve $\boldsymbol\Omega$ and $\boldsymbol z$ in a closed form without further restriction becasue of merely one known item.

## Stationary Time-Series Source Separation Using Autocovariance Matrices

In information processing, the signals are usually recorded by highly precise measurement instruments at a given time interval, and thus arose the natural time-series data structure. Assuming pre-centered data, the time series observations can be easily adapted to the basic form \@ref(eq:SOM) by introducing the temporal subscript of $t=1,2,\dots, T$ as $\boldsymbol{x}_t = \boldsymbol{\Omega z}_t$. Focusing on the autocovariance matrices, a second order statistics, the second order source separation (SOS) model seeks to extract the original source signals based mainly on independence. Furthermore, the source signals are assumed to be weekly stationary, implying that the autocovariance matrices vary only on the lag $\tau$ but not on the time of observation $t$, which mathematically express as $E(\boldsymbol z_t \boldsymbol z_{t+\tau}')$ are constant given $\tau$ for all $t=1,2,\dots,T$. The SOS model [adpated @miettinen2016separation] is,

\begin{equation}
\begin{aligned} 
\boldsymbol x_t = \boldsymbol{\Omega}\boldsymbol z'_t,\ \ &\ t=1,2,\dots,T
\\ \text{satisfies }(1)\ & E( \boldsymbol z_t) = \boldsymbol 0 
\\ (2)\ & E( \boldsymbol z_t \boldsymbol z_t') = \text{Cov}( \boldsymbol z_t) = \boldsymbol I_p
\\ (3)\ & E( \boldsymbol z_t \boldsymbol z_{t+\tau}') = \boldsymbol\Lambda_\tau \text{ diagonal for all } \tau = 1,2,\dots
\end{aligned}    
(\#eq:SOS)
\end{equation}

This semi-parametric model can be solved by joint optimization for the diagonal properties in assumption 3 under the restriction of assumption (2) in the SOS model [@miettinen2016separation.] [DETAILS MAY NEED FURNISH].

Nordhausen [-@nordhausen2014robustifying] expanded the algorithm to non-stationary time series using locally stationary intervals and further robustifying with average spatial-sign autocovariances on such intervals.

## Introducing Time Varying Factor into SOBI

[To be adapted from version 1]




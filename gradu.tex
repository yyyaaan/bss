\documentclass[utf8,english]{gradu3}
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{bm}
\usepackage{booktabs}
\usepackage[bookmarksopen,bookmarksnumbered,linktocpage]{hyperref}
\addbibresource{ref.bibtex} % The file name of your bibliography database

\begin{document}

\title{Time-Varying Source Separation using Covariances and Joint Diagnolization }
\translatedtitle{\LaTeX-tutkielmapohjan {gradu3} käyttö}
\studyline{Statistics}
\avainsanat{Sokealähdeerotus, SOBI, TV-SOBI}
\keywords{Blind source separation, SOBI, TV-SOBI}
\tiivistelma{%
  Sokealähdeerotus
}
\abstract{%
  Blind source separation (BSS) seeks to recover the true signals from the only observed values, the multivariate time-series mixture, and usually no prior information (blind) about the mixing matrix is available. There are various methodologies established to solve the BSS problems, and notably SOBI seeks to identify sources through the spatial independence in second order statistics. This paper extends the Second Order Source Separation (SOS) model   in terms of hidden time variation in mixing, as initially introduced by Yeredor (2003), and presents JD-TV-SOBI procedure aiming to estimate through joint diagonalization the time-varying unmixing matrices and ultimately derives the latent independent sources. The generalized JD-TV-SOBI will be covered in non-linear time variation and non-stationary source signals, and finally attempts towards the corresponding unsupervised machine learning with tensorial time-series. The performance of JD-TV-SOBI will be analyzed with simulated data and compared to other most common BSS methods. 
}

\author{Yan Pan}
\contactinformation{\texttt{yan@yan.fi}}
\supervisor{Sara Taskinen}

\maketitle

\mainmatter

\chapter{Introduction}

In certain real-world situation, there exists strong need for extracting structured information from observable mixtures of unknown signals. For example, a recording of speech may contain external noise from nearby road traffic, minor discussion among audience and constant electronic interference in addition to the speech voice itself (the "cocktail party" problem). Independence is usually one of the important statistical properties of desired processed information.

Blind Source Separation is set of unsupervised machine learning algorithms in terms of input being only a single data matrix. The output is usually not anticipated in advance and such algorithms mostly serve as exploratory purposes (\cite{hyvarinen2013independent}). 

\chapter{Second Order Blind Source Separation}

Blind Source Separation (BSS) assumes that the observed information $p$-vector $\boldsymbol{x}$ is a static mixture of $p$-variate latent source vector $\boldsymbol{z}$, and the basic blind source model can be written as $\boldsymbol{x} = \boldsymbol{\mu} + \boldsymbol{\Omega z}'$, where $\boldsymbol{\Omega}$ denotes $p\times p$ mixing matrix, and $\boldsymbol{\mu}$ stands for the location (usually mean) of $\boldsymbol{x}$. This model can be easily expanded to multidimensional observations such that the $n\times p$ matrices $\boldsymbol{x}=(\boldsymbol{x}_1, \boldsymbol{x}_2,\dots,\boldsymbol{x}_n)'$ and $\boldsymbol{z}=(\boldsymbol{z}_1, \boldsymbol{z}_2,\dots,\boldsymbol{z}_n)'$. Without loss of generality, it can be assumed that the source signals are zero mean (in the sense that each column-vector is zero mean), and the model can be further simplified if the observations are assumed to be zero mean, which can be achieved by subtracting the location parameter, i.e. $(\boldsymbol{x} - \boldsymbol{\mu}) = \boldsymbol{\Omega z}$. This paper specifies the observations and signals matrices to be $n\times p$ organized in the way that each $p$-vector observation or signal is recorded as a row-vector, which is consistent with common data science programming R and Python Pandas. Finally, the basic form is \footnote{For notations in this paper, vectors and matrices are always using bold symbols. The operator $\cdot'$ means matrix transpose},
\begin{equation}
\label{eq:SOM}
    \underset{(n\times p)}{\boldsymbol x} = \underset{(p\times p)}{\boldsymbol{\Omega}} \ \underset{(n\times p)}{\boldsymbol{z}}'
\end{equation}
Although not mathematically required, it is assumed that $\boldsymbol{\Omega}$ is a full-ranked $p\times p$ matrix. In fact, an decrease in dimension of $\boldsymbol{\Omega}$ will allow fewer signal components (number of columns in $\boldsymbol z$) generated from the same observation $\boldsymbol{x}$. However, it is not necessary due to fact that the output carries real-world information, and the usefulness of signal components (columns) can be better determined using scientific evidence of the signal itself. Consequently, it is not 

The major second order blind source separation approaches include AMUSE (Algorithm for Multiple Unknown Signals Extraction) by \citeauthor{tong1990amuse} (\citeyear{tong1990amuse}) and SOBI (Second Order Blind Identification) originally proposed by \citeauthor{belouchrani1997blind} (\citeyear{belouchrani1997blind}).

\section{Time-Series Source Separation using Autocovariance Matrices}



\citeauthor{nordhausen2014robustifying} (\citeyear{nordhausen2014robustifying}) expanded the algorithm to non-stationary time series using locally stationary intervals and further robustifying with average spatial-sign autocovariances on such intervals.

\printbibliography

\end{document}

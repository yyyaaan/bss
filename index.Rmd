---
title: TV-SOBI Algorithm with R
bibliography: ref.bibtex
nocite: | 
  @hyvarinen2000independent,
  @miettinen2017blind,
  @hyvarinen2013independent,
  @baloch2005robust,
  @miettinen2016separation,
  @virta2017blind,
  @yeredor2003tv 
---

Time-varying second order blind source separation.

<i>The R implementation code here is to step by step in mathematical description, and the `tvSOBI` function (see the later sections in "R-Lab") is more optimize as a resuable function. The example illustrated below is cached, and further simulation suggests that the performance of TV-SOBI varies greatly. R-Lab provides a framework to evaluate.</i>

## Concept of BSS

$$\begin{aligned} 
\boldsymbol x_t =  \boldsymbol \Omega_t \boldsymbol  z_t
\\ & \begin{cases}  
\text{zero-mean} & E( \boldsymbol z_t) = \boldsymbol 0 ,\ \ \forall t \text{ pre-centered}
\\ \text{independence} & \text{components of } \boldsymbol z \text{ are uncorrelated}
\\ \text{stationary} & \text{the process } ( \boldsymbol z_t)_{t\in \mathbb Z} \text{ weak stationary time-series}
\\ \text{time-varying mixing}& \boldsymbol \Omega_t =  ( \boldsymbol I + t \boldsymbol{\mathcal E})\boldsymbol \Omega_0 
\\ \text{slow time-variation} & \boldsymbol{\mathcal E} << \boldsymbol I
\end{cases} 
\end{aligned}$$

## TV-SOBI Problem Formulation

$$\begin{aligned} 
\boldsymbol x_t =  ( \boldsymbol I + t \boldsymbol{\mathcal E})\boldsymbol \Omega_0 \boldsymbol  z_t
\\
\\ (1.1)\ & E( \boldsymbol z_t) = \boldsymbol 0 
\\ (1.2)\ & E( \boldsymbol z_t \boldsymbol z_t') = \text{Cov}( \boldsymbol z_t) = \boldsymbol I
\\ (2.1)\ & E( \boldsymbol z_t \boldsymbol z_{t+\tau}') = \boldsymbol\Lambda_\tau \text{ diagonal for all } \tau = 1,2,\dots
\\
\\ \text{Given } \boldsymbol x_t, \text{ optimize } \boldsymbol\Omega_0 \text{ and } \boldsymbol{\mathcal E}  &\text{ for (2.1) under restricstions of (1.1) and (1.2)}
\end{aligned}$$

<blockquote>
Simulate data with `stats::arima.sim`, where source signals are scaled to be zero-mean and unit-variance. For better performance, a relatively small $N$ and $\tau$ (lag.max) is selected.
</blockquote>

```{r sim, eval=F}
N <- 1e3

omega <- matrix(rnorm(9) , ncol = 3)
epsilon <- matrix(rnorm(9) * 1e-3, ncol = 3)

z1 <- arima.sim(list(ar=c(0.3,0.6)),N)
z2 <- arima.sim(list(ma=c(-0.3,0.3)),N)
z3 <- arima.sim(list(ar=c(-0.8,0.1)),N)
z <- apply(cbind(z1,z2,z3), 2, scale)

X <- matrix(nrow = nrow(z), ncol = ncol(z))
for (i in 1:N) X[i,] <- z[i,] %*% t(omega) %*% t(diag(3) + i * t(epsilon))
plot.ts(X, main = "Time-Varying Mixture")
```

```{r sim2, echo=F, eval = T}
# save(N,omega,epsilon, z, X, file = "cache.rdata")
load("cache.rdata")
plot.ts(X, main = "Time-Varying Mixture")
```

## Autocovariances

$$\begin{aligned}
\text{Cov}( \boldsymbol x_t , \boldsymbol x_{t+\tau}) &= E(\boldsymbol x_t \boldsymbol x_{t+\tau}')
\\ & = E[( \boldsymbol I + t \boldsymbol{\mathcal E})\boldsymbol \Omega_0 \boldsymbol  z_t\ \boldsymbol  z_{t+\tau} ' \boldsymbol \Omega_0 '( \boldsymbol I + t \boldsymbol{\mathcal E}')]
\\ &= \underbrace{\boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0'}_{:= \boldsymbol{Ra}_\tau}
 + t (\underbrace{ \boldsymbol{\mathcal E } \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' +  \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'} _{:= \boldsymbol{Rb}_\tau} )
 + t^2 (\underbrace{\boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'}_{:= \boldsymbol{Rc}_\tau})
 + \underbrace {\tau (\boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0')
 + t \tau (\boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}')}_\text{ignorable}
\\ &\approx \boldsymbol{Ra}_\tau + t\, \boldsymbol{Rb}_\tau + t^2\, \boldsymbol{Rc}_\tau
\end{aligned}$$

Then, the solutions for $\boldsymbol\Omega_0$ and $\boldsymbol{\mathcal E}$ can be found through the equation group

$$\begin{cases}
E(\boldsymbol x_t \boldsymbol x_{t+\tau}') =  \boldsymbol{Ra}_\tau + t\, \boldsymbol{Rb}_\tau + t^2\, \boldsymbol{Rc}_\tau  & \text{(1) linear model}\\
\boldsymbol{Ra}_\tau = \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0'  & \text{(2a) optimization} \\
\boldsymbol{Rb}_\tau = \boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' +  \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}' & \text{(2b) optimization}\\ 
\boldsymbol{Rc}_\tau = \boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'  & \text{(2c) optimization} \end{cases}$$

<blockquote>
Autocovariance matrices at different lags can use `stats::acf` function (`stats` are pre-installed and pre-loaded packages in `R`). Considering the algorithm, __it is not necessary to calculate the autocovariance matricies__ at all.
</blockquote>

## Algorithm

### Step 1 Linear Regression

Estimate $\boldsymbol {Ra}_\tau, \ \boldsymbol {Rb}_\tau, \ \boldsymbol {Rc}_\tau$ through a linear model (__element-wise__) with the empirical autocovariance.

$$\begin{aligned} 
\text{for each desired lags } \tau = 1,2,\dots, L &
\\ \text{for each } i = 1,2,\dots, & p
\\ \text{for each } j = & 1,2,\dots, p
\\ 
\\ \underbrace {\begin{bmatrix} E(\boldsymbol x_{1} \boldsymbol x'_{1+\tau})[i,j] \\ E(\boldsymbol x_{2} \boldsymbol x'_{2+\tau})[i,j] \\ E(\boldsymbol x_{3} \boldsymbol x'_{3+\tau})[i,j]  \\ \vdots \\ E(\boldsymbol x_{n-\tau} \boldsymbol x'_{n})[i,j] \end{bmatrix}}_{ \boldsymbol y_\tau [i,j]}
 &= \underbrace{ \begin{bmatrix}  1 & 1 & 1^2 \\ 1 & 2 & 2^2\\  1 & 3 & 3^2\\ \vdots & \vdots & \vdots \\  1 & (n-\tau) & (n-\tau)^2 \end{bmatrix}  }_{ \boldsymbol H} \ 
 \underbrace{ \begin{bmatrix} \boldsymbol {Ra}[i,j] \\ \boldsymbol{Rb}[i,j] \\ \boldsymbol {Rc} [i,j] \end{bmatrix}  }_{ \boldsymbol \theta_\tau [i,j]} 
\\ \\ \text{LS/ML-estimator gives }
\\ \begin{bmatrix} \boldsymbol {Ra}[i,j] \\ \boldsymbol {Rb}[i,j] \\ \boldsymbol {Rc}[i,j] \end{bmatrix} &= ( \boldsymbol H ' \boldsymbol H)^{-1} \boldsymbol H' \boldsymbol y_\tau[i,j]
\end{aligned}$$

The definition of $\boldsymbol{Ra},\boldsymbol{Rb},\boldsymbol{Rc}$ implies the symmetric structure, while the linear estimator does not. Thus, a simple trick is applied as $\boldsymbol{Ra} \leftarrow(\boldsymbol{Ra} + \boldsymbol{Ra}')/2$.

```{r step1, results="hold"}
X <- scale(X, center = T, scale = F) # params X, re-scale not really needed here
lag.max <- 6 # params
p <- ncol(X); n <- nrow(X)

Ra <- Rb <- Rc <- array(dim = c(p, p, lag.max+1))
for (lag in 0:lag.max) {
  for (i in 1:p){
    for(j in 1:p){
      y.design <- X[(1+lag):n,i] * X[1:(n-lag),j] # empirical autocovariance
      h.design <- cbind(rep(1, n-lag), 1:(n-lag), (1:(n-lag))^2)
      est <- lm(y.design ~ h.design - 1)$coefficients # using lm avoids the singular problem
      Ra[i, j, lag+1] <- est[1]
      Rb[i, j, lag+1] <- est[2]
      Rc[i, j, lag+1] <- est[3]
    }
  }
}

for (i in 1:(lag.max+1)) { # symmetrize
  Ra[,,i] <- (Ra[,,i] + t(Ra[,,i]))/2 
  Rb[,,i] <- (Rb[,,i] + t(Rb[,,i]))/2
  Rc[,,i] <- (Rc[,,i] + t(Rc[,,i]))/2
}

cat("\nsome abstract of Ra"); print(Ra[,,1:2])
cat("\nsome abstract of Rb"); print(Rb[,,1:2])
cat("\nsome abstract of Rc"); print(Rc[,,1:2]) # Ra >> Rb >> Rc
```

### Step 2 Sub-SOBI procedure for $\boldsymbol W_0 = \boldsymbol {C\Omega_0} ^{-1}$

$\boldsymbol {Ra}$ can be viewed as the autocovariance of another p-variate time series, and the estimation follows exactly the SOBI.Suppose the unmixing matrix $\boldsymbol W_0 = \boldsymbol\Omega_0 ^{-1}$.

__ISSUE__ Unlike (auto)covariance matrix, $\boldsymbol{Ra}$ may NOT be positive semi-definite, which makes whitening impossible.

$$\begin{aligned}
\text{Whitening } \tilde {\boldsymbol{Ra}}_\tau &= \boldsymbol{Ra}_0^{-1/2}\ \boldsymbol {Ra} _\tau\ \boldsymbol {Ra}_0^{-1/2\ '}
\\
\\ \text{JD for orthognal } \boldsymbol V &\begin{cases} 
\boldsymbol V \tilde {\boldsymbol R}_1^{(0)} \boldsymbol V' & =  \boldsymbol\Lambda_1 \\
\boldsymbol V \tilde {\boldsymbol R}_2^{(0)} \boldsymbol V' & =  \boldsymbol\Lambda_2 \\
&\vdots \\
\boldsymbol V \tilde {\boldsymbol R}_3^{(0)} \boldsymbol V' & =  \boldsymbol\Lambda_3 \\
\end{cases}
\\
\\ \text{Finally, } \boldsymbol W_0 &= \boldsymbol V \boldsymbol Ra_0^{-1/2}
\\ \Leftrightarrow \boldsymbol\Omega_0 &= \boldsymbol Ra_0^{1/2} \boldsymbol V
\end{aligned}$$

<blockquote>
A function is created to combine the above procedures, i.e. first whitening then joint-diagnolizing (this may be useful later). This function solves for square matrix (not orthogonal) that makes $\Lambda_i = W'A_i W$ as diagonal as possible.
</blockquote>

```{r step2}
myCovJD <- function(covs){
  require(JADE)
    # whitening using S_0^{-1/2}, through eigen calculation
  eig <- eigen(covs[,,1])
  white <- solve(eig$vectors %*% sqrt(diag(eig$values)))
    # whiten and format to rjd
  covs.white <- array(dim = c(dim(white),lag.max))
  for (i in 1:lag.max) covs.white[,,i] <- white %*% covs[,,i+1] %*% t(white)
    # joint diagnolization for V | note for the transpose of V
  jd <- frjd(covs.white, maxiter = 1e5)
    # D is the estimated diagnals
  list(W = t(jd$V) %*% white, D = jd$D, white = white)
}
  # now it is easy
jd <- myCovJD(Ra)
W.est <- jd$W
omega.est <- solve(W.est)
W.est
cat("Performance Index of Omega (MDI):", JADE::MD(W.est, omega),
    "\nCompared with SOBI:", JADE::MD(SOBI(X)$W, omega))
```

### Step 3 Method 1&2 Estimate $\boldsymbol{\mathcal E}$ via $\boldsymbol{Rb}$

This step can be transformed to a linear optimization problem after designing proper matricies based on pervious results; the ML/LS-estimator give the estimator as in Step 1. Method 1 and 2 are very similar, and the only difference is the choice of $\boldsymbol Q$. Method 2 use the results from joint diagonalization in the previous steps.

$$\begin{aligned}
\text{sum for } \tau = 1,2,\dots,L
\\ \text{vec}( \boldsymbol {Rb}_\tau) &= \big( \underbrace{\boldsymbol H_{\tau,1} + \boldsymbol H_{\tau,2} }_{ \boldsymbol H} \big) \text{vec}( \boldsymbol{\mathcal E})  
\\ \text{where } &   \boldsymbol H \text{ are carefully designed transformation based on } \boldsymbol Q
\\ & \begin{cases}
\text{Method 1} & \boldsymbol Q= \boldsymbol {Ra}_\tau 
\\  \text{Method 2} & \boldsymbol Q=  \hat{\boldsymbol\Omega_0}\ \hat{ \boldsymbol\Lambda_\tau} \ \hat{\boldsymbol\Omega_0}'
\end{cases}
\end{aligned}$$

<blockquote>
In R implementation, a handy function is written for constructing the design matrix $\boldsymbol H$ because the design is the same with 2 methods. Attention is needed for vectorization by row or by column ($y.design$ by column, $\boldsymbol{\mathcal E}$ by row).

The accuracy measurement is not clear (see "issues" page)
</blockquote>

```{r step3a}
designH <- function(Q){
  lag.max <- dim(Q)[3] - 1; p <- dim(Q)[1]
  H1 <- H2 <- array(0, dim = c(p^2, p^2, lag.max + 1))
  for(lag in 0:lag.max){
    for(i in 1:p^2){
      # the column to use  
      Qi <- Q[,ceiling(i/p),lag+1]
      # H1 similar to diagnal
      pos <- (ifelse(i%%p == 0, p, i%%p ) - 1) * p  + (1:p)
      H1[i, pos, lag+1] <- Qi
      # H2 similar to vec
      pos <- (ceiling(i/p) - 1) * p + (1:p)
      H2[i, pos, lag+1] <- Qi
    }
  }
  h.design <- array(0, dim = dim(H1)[1:2])
  for (i in 0:lag.max) h.design <- h.design + H1[,,i+1] + H2[,,i+1]
  h.design
}

y.design <- matrix(0, nrow = p^2) # a vector
for (i in 0:lag.max) y.design <- y.design + matrix(as.vector(Rb[,,i+1]), nrow = p^2)

# method 1
est <- lm(y.design ~ designH(Ra) - 1)$coefficients
epsilon.est1 <- matrix(est, nrow = p, byrow = T)
epsilon.est1

# method 2
Q <- array(dim = c(p, p, lag.max + 1))
Q[,,1] <- omega.est %*% t(omega.est)
for (lag in 1:lag.max) Q[,,lag + 1] <- omega.est %*% jd$D[,,lag] %*%  t(omega.est)

est <- lm(y.design ~ designH(Q) - 1)$coefficients
epsilon.est2 <- matrix(est, nrow = p, byrow = T)
epsilon.est2
```

### Step 3 Method 2 Estimate $\boldsymbol{\mathcal E}$ via $\boldsymbol{Rc}$

The third approach may potentially be less accurate due to the use of (2c), which are supposed to carry less information. It is then a joint diagonalization problem searching for $\boldsymbol{\mathcal E \Omega_0}$. Note that it faced challenge as in step 2 with regards to the postive definite property.

$$\boldsymbol R_\tau^{(2)} = \boldsymbol{\mathcal E} \boldsymbol\Omega_0 \boldsymbol\Lambda_\tau \boldsymbol \Omega_0' \boldsymbol{\mathcal E}'$$

<blockquote>
This joint diagnolization is exactly the same as step 2, and the only extra task is to recover $\boldsymbol{\mathcal E}$ from $\boldsymbol{\mathcal E\Omega}_0$ using $\hat{ \boldsymbol\Omega}_0$.
</blockquote>

```{r step3b}
# this is an example of failed jd 
try(jd <- myCovJD(Rc))
# here W = (Epsilon * Omega)^{-1} = W * Epsion^{-1}
# epsilon.est3 <- solve(jd$W) %*% W.est
```

### Step 3 Method 3 Analytically Solve for $\boldsymbol{\mathcal E}$

Furthermore, observe that it can be solve analytically (in theory) by the formula given in (2b) after element-wise accumulation over $\tau=1,2,\dots,L$. The result should be the same as method 1.

$$ \sum\limits_{\tau=1}^L \boldsymbol {Rb}_\tau = \boldsymbol{\mathcal E} \bigg( \sum\limits_{\tau=1}^L \boldsymbol{Ra}_\tau \bigg) + \bigg( \sum\limits_{\tau=1}^L \boldsymbol{Ra} _\tau'\bigg) \boldsymbol{\mathcal E'}$$


##R-Notes

The structure of data matrix is in fact transposed ($n\times p$ in `R`). 
<br />
Vectorization and matrix creation are by column with default settings.

## References

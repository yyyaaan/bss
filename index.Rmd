---
title: "Selected Proof with Details"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Model and Notations

$$\begin{aligned} 
(T\times p) & &(T\times p) & (p\times p)
\\ \boldsymbol x_t &= &\boldsymbol z_t & \boldsymbol\Omega'_t
\\ &= & \boldsymbol z_t & \boldsymbol\Omega'( \boldsymbol I+t \boldsymbol{\mathcal E})'
\end{aligned}$$

Key assumptions: uncorrelated, zero-mean, independence of parameters, stationary. Linearly varying mixture $\boldsymbol\Omega_t = ( \boldsymbol I+t \boldsymbol{\mathcal E}) \boldsymbol\Omega$


- $\boldsymbol \Omega$ is the initial mixing matrix, a shorthand for $\boldsymbol \Omega_0$; 
- $\boldsymbol{\mathcal E}$ a time-varying factor; 
- $t=1,2,\dots, T$ time-index;
- $l\in L \subseteq \mathbb Z^+$ selected lag for time. It is common to choose $L=\{1,2,3,...\}$
- Let $\boldsymbol \Lambda_{t,l} = \mathbb E( \boldsymbol z'_t \boldsymbol z'_{t+l})$ is the covariance matrix of the source signals (due to and zero-mean). Further, because of stationarity, $\boldsymbol\Lambda_l = \boldsymbol\Lambda_{t,l}$ for all $t$.

The underline is used to illustrate that the elements are considered as a whole and does not have mathematical influence.

Notations: $\boldsymbol x'$ stands for the matrix transpose of $\boldsymbol x$. Time-index $t=1,2,\dots, T$. All bold symbols stands for matrices (incl. vectors) and non-bold ones are real numbers, except for the set of real numbers $L$. Necessary matrix calculation includes [vectorization](https://en.wikipedia.org/wiki/Vectorization_(mathematics)) ($\text{vec}$), [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) ($\otimes$) and $p^2 \times p^2$ [commutation matrix](https://en.wikipedia.org/wiki/Commutation_matrix) ($\boldsymbol K^{(p,p)}$).

## Autocovariance matrix

$$\begin{aligned} 
\mathbb E( \boldsymbol x'_t \boldsymbol x_{t+l} )
&= \mathbb E \bigg( (1+t \boldsymbol{\mathcal E})\ \boldsymbol\Omega\ \boldsymbol z'_t\ \boldsymbol z_{t+l}\ \boldsymbol\Omega'\ (1+ (t+l) \boldsymbol{\mathcal E})'  \bigg) (p\times p)
\\ &= (1+t \boldsymbol{\mathcal E})\ \boldsymbol\Omega\ \mathbb E(\boldsymbol z'_t\ \boldsymbol z_{t+l})\ \boldsymbol\Omega'\ (1+ (t+l) \boldsymbol{\mathcal E})' &(1)
\\ &=(1+t \boldsymbol{\mathcal E})\ \boldsymbol\Omega\ \boldsymbol\Lambda_l  \ \boldsymbol\Omega'\ (1+ (t +l) \boldsymbol{\mathcal E})' &(2)
\\ &=(1+t \boldsymbol{\mathcal E})\ \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega'  \ (1+ (t +l) \boldsymbol{\mathcal E})' &(3)
\\ &= \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + t( \underline{\boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) + t^2(\underline{ \boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) + tl ( \underline{\boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) + l(\underline{ \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'})
\end{aligned}$$

Above (1) is because of parameter-independence; (2) and (3) are defined notations. Further, observe that $\boldsymbol\Lambda_l$ and $\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega'$ are symmetric positive semi-definite matrices (followed by autocovariance and $A=U'U \Leftrightarrow A \succeq 0$).

For all $l\in L$, 

$$\begin{cases} \boldsymbol x_1' \boldsymbol x_{1+l} = \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + 1  (\underline{\boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) + 1(1 + l) \underline{ \boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'} + l \underline{\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'} \\ \boldsymbol x_2' \boldsymbol x_{2+l} = \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + 2  (\underline{\boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) + 2(2 + l) \underline{ \boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'} + l \underline{\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'} \\ \vdots \\ \boldsymbol x_{T-l}' \boldsymbol x_{T} = \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + (T-l)  (\underline{\boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) + (T-l)T\, \underline{ \boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'} + l \underline{\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'} \end{cases}$$

## Step 1: Separation of Autocovariance Structure

Consider element-wise equivalence, for $i,j=1,2,\dots, p$, the above equivalent to 

$$
\underbrace{\begin{bmatrix} \boldsymbol x_1' \boldsymbol x_{1+l}\ [i,j] \\ \boldsymbol x_2' \boldsymbol x_{2+l}\ [i,j] \\ \vdots \\ \boldsymbol x_{T-l}' \boldsymbol x_{T}\  [i,j]\end{bmatrix}}_{:= \mathbf S_l}
= \underbrace {\begin{bmatrix} 1 & 1 & 1^2+l &l\\  1 & 2 & 2^2+2l &l \\ \vdots &\vdots &\vdots &\vdots\\  1 & T-l & (T-l)^2+(T-l)l &l \end{bmatrix}} _ {:= \mathbf H_l}  
\underbrace {\begin{bmatrix} (\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega') \ [i,j]\\ ({\boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'})\ [i,j]\\ ({ \boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) \ [i,j]  \\ ( {\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) \ [i,j]\end{bmatrix}} _ {:= \boldsymbol \beta_l}
$$


If $l$ is __fixed__, note that the coefficients of $\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'$ are a constant. Therefore, the linear system is better written as,

$$
\underbrace{\begin{bmatrix} \boldsymbol x_1' \boldsymbol x_{1+l}\ [i,j] \\ \boldsymbol x_2' \boldsymbol x_{2+l}\ [i,j] \\ \vdots \\ \boldsymbol x_{T-l}' \boldsymbol x_{T}\  [i,j]\end{bmatrix}}_{:= \mathbf S_l}
= \underbrace {\begin{bmatrix} 1 & 1 & 1(1+l) \\  1 & 2 & 2(2+l) \\ \vdots &\vdots &\vdots &\\  1 & T-l & (T-l)T \end{bmatrix}} _ {:= \mathbf H_l}  
\underbrace {\begin{bmatrix} (\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega'  +l \, {\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) \ [i,j]\\ ({\boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'})\ [i,j]\\ ({ \boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) \ [i,j] \end{bmatrix}} _ {:= \boldsymbol \beta_l}
$$

Vectorization and stacking of all $l\in L$ can greatly improve efficiency and accuracy.

$$
\text{(vectorization) } \underbrace{\begin{bmatrix} \text{vec}(\boldsymbol x_1' \boldsymbol x_{1+l}) \\ \text{vec}(\boldsymbol x_2' \boldsymbol x_{2+l}) \\ \vdots \\ \text{vec}(\boldsymbol x_{T-l}' \boldsymbol x_{T})\end{bmatrix}}_{:= \mathbf S_l}
= \underbrace {\begin{bmatrix} 1 & 1 & 1^2+l \\  1 & 2 & 2^2+2l \\ \vdots &\vdots &\vdots \\  1 & T-l & (T-l)T \end{bmatrix} \otimes \boldsymbol I_{p^2}\ \ } _ {:= \mathbf H_l \otimes \boldsymbol I_{p^2}}  
\underbrace {\begin{bmatrix} \text{vec}(\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega') + l\ \text{vec}({\boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'})\\ 
\text{vec}({\boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' + \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'})\\ 
\text{vec}({ \boldsymbol{\mathcal E}\, \boldsymbol\Omega\boldsymbol\Lambda_l  \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}) \end{bmatrix}} _ {:= \boldsymbol \beta_l}
$$


The linear estimation leads to the matrix equations,

$$ \text{for all } l \in L:\ 
\begin{cases} 
\widehat {\boldsymbol\beta_{1,l}} = \boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega'  +l \, {\boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega'\, \boldsymbol{\mathcal E}'} \\
\widehat {\boldsymbol\beta_{2,l}} = {\boldsymbol{\mathcal E}\, \boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega' + \boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega'\, \boldsymbol{\mathcal E}'}\\
\widehat {\boldsymbol\beta_{3,l}} ={ \boldsymbol{\mathcal E}\, \boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega' \, \boldsymbol{\mathcal E}'}
\end{cases}
$$

Note that $\boldsymbol\beta_2$ and $\boldsymbol\beta_3$ are surely symmetric, and symmetry-fix is applied.

## Step 2 (Method A): Using $\boldsymbol \beta_1$ and $\boldsymbol \beta_2$

This approaches focus on the numerically large items, i.e. ignoring the small valued $\boldsymbol \beta_3$.

### Find representation of $\boldsymbol{\Omega \mathcal E \Omega}'$

Observing the equality of $\widehat {\boldsymbol\beta_{1,l}}$ and $\widehat {\boldsymbol\beta_{2,l}}$,

$$\begin{aligned}
\widehat {\boldsymbol\beta_{1,l}} + \widehat {\boldsymbol\beta_{1,l}}'
&= \boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega'  +l \, {\boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega'\, \boldsymbol{\mathcal E}'} + \boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega'  +l \, {\boldsymbol{\mathcal E} \boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega' } 
\\ &= 2 \boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega' + l\, \widehat {\boldsymbol\beta_{2,l}}
\end{aligned}$$

### Approximated Joint Diagonalization for $\boldsymbol\Omega$

The above ensures that for each $l \in L,\ \boldsymbol\Omega\ \boldsymbol\Lambda_l \ \boldsymbol\Omega' = \frac 1 2 \bigg( \widehat {\boldsymbol\beta_{1,l}} + \widehat {\boldsymbol\beta_{1,l}}' - l \widehat {\boldsymbol\beta_{2,l}} \bigg)$, which provides a series of matrices for Joint Diagonalization. Consequently, solutions are available for,

$$
\begin{cases} \widehat{\boldsymbol\Omega} \\ \widehat{\boldsymbol\Lambda_l} \text{ for all } l\in L \end{cases}
$$

### Solve for $\boldsymbol {\mathcal E}$ with $\widehat {\boldsymbol\beta_{2}}$

The estimation from step 1 says that $\widehat {\boldsymbol\beta_{2, l}} \approx \boldsymbol{\mathcal E} \widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'} + \widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'} \boldsymbol{\mathcal E}'$. The vectorization form is,

$$\begin{aligned} 
\text{vec}( \widehat {\boldsymbol\beta_{2, l}})
&= \text{vec}(\boldsymbol{\mathcal E} \widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'})+ \text{vec}(\widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'} \boldsymbol{\mathcal E}')
\\ &= \text{vec}( \boldsymbol I \boldsymbol{\mathcal E} (\widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'}))+ \text{vec}((\widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'}) \boldsymbol{\mathcal E}' \boldsymbol I)
\\ &= \bigg((\widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'})' \otimes \boldsymbol I \bigg) \text{vec}( \boldsymbol{\mathcal E}) + \bigg( \boldsymbol I' \otimes (\widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'}) \bigg) \text{vec}( \boldsymbol{\mathcal E}')
\\ &= \bigg((\widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'}) \otimes \boldsymbol I \bigg) \text{vec}( \boldsymbol{\mathcal E}) + \bigg( \boldsymbol I \otimes (\widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'}) \bigg) \boldsymbol K ^{(p,p)}\text{vec}( \boldsymbol{\mathcal E})
\\ &= \bigg((\widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'}) \otimes \boldsymbol I + (\boldsymbol I \otimes (\widehat{\boldsymbol\Omega} \widehat{ \boldsymbol\Lambda_l} \widehat{ \boldsymbol\Omega'}) ) \boldsymbol K ^{(p,p)}\bigg ) \text{vec}( \boldsymbol{\mathcal E})
\end{aligned}$$

In computation, stacking (row-binding) over all $l\in L$ solves the unique solution of $\widehat{ \boldsymbol{\mathcal E}}$. The solution can be achieved through inverse, but considering the possibility of singularity, the linear estimation form is more feasible.

## Step 2 (Method B): Using $\boldsymbol \beta_2$ and $\boldsymbol \beta_3$

This methods starts from the relatively "small" element. Only $\boldsymbol \beta_3$ and $\boldsymbol \beta_2$ are used. It might be more accurate if $\boldsymbol{\mathcal E}$ is not sufficiently small.

### Approximated Joint Diagnolization of $\boldsymbol{\mathcal E \Omega}$

Observing that $\boldsymbol\beta_3 = \boldsymbol{\mathcal E \Omega \Lambda}_l \boldsymbol{\Omega' \mathcal E'}$, it follow the joint diagnolization algorithm, and the approximated solutions are available for

$$
\begin{cases} \widehat{\boldsymbol{ \mathcal E \Omega}} \\ \widehat{\boldsymbol\Lambda_l}  \text{ for all } l\in L  \end{cases}
$$

### Solving for $\boldsymbol{\mathcal E}$ and $\boldsymbol \Omega$

Using the estimation from step 1, $\widehat {\boldsymbol\beta_{2,l}} \approx \widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l} \ \boldsymbol\Omega' + \boldsymbol\Omega\ \widehat{\boldsymbol\Lambda_l} (\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega})'= \widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega \boldsymbol\Lambda_l}\, \boldsymbol\Omega '+ \boldsymbol\Omega (\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega \boldsymbol\Lambda_l})'$. Denote $\boldsymbol A = \widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega \boldsymbol\Lambda_l}$ for convenience, it follows by vectorization,

$$\begin{aligned} \text{vec}(\widehat{\boldsymbol\beta_{2,l}}) &= \text{vec}( (\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l}) \boldsymbol{\Omega}') + \text{vec}( \boldsymbol{\Omega} (\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l})')
\\ &= \text{vec}( \boldsymbol{A\Omega' I'}) + \text{vec}( \boldsymbol{I \Omega} (\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l})')
\\ &= (\boldsymbol I \otimes (\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l})) \text{vec}( \boldsymbol \Omega') + ((\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l}) \otimes \boldsymbol I) \text{vec}( \boldsymbol \Omega)
\\ &= (\boldsymbol I \otimes (\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l})) \boldsymbol K ^{(p,p)}\text{vec}( \boldsymbol \Omega) + ((\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l}) \otimes \boldsymbol I) \text{vec}( \boldsymbol \Omega)
\\ &= \bigg( (\boldsymbol I \otimes (\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l})) \boldsymbol K ^{(p,p)} + (\widehat{\boldsymbol{\mathcal E}\, \boldsymbol\Omega}\ \widehat{\boldsymbol\Lambda_l}) \otimes \boldsymbol I \bigg) \text{vec}( \boldsymbol \Omega)
\end{aligned}$$

It is easy to stack (row-binding) both side of the equation over $l\in L$. A simple linear model solves $\widehat{\boldsymbol\Omega}$ and thereafter $\widehat{\boldsymbol{\mathcal E}} = \widehat{\boldsymbol{\mathcal E \Omega}} (\widehat{\boldsymbol\Omega})^{-1}$.
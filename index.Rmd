---
title: "Selected Proof with Details"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Model and Notations

$$\begin{aligned} 
(n\times p) & &(n\times p) & (p\times p)
\\ \boldsymbol x_t &= &\boldsymbol z_t & \boldsymbol\Omega'_t
\\ &= & \boldsymbol z_t & \boldsymbol\Omega'( \boldsymbol I+t \boldsymbol{\mathcal E})'
\end{aligned}$$

Key assumputions: uncorrelated, zero-mean, independence of parameters, stationary. $\boldsymbol\Omega_t = ( \boldsymbol I+t \boldsymbol{\mathcal E}) \boldsymbol\Omega$

Notations: $\boldsymbol x'$ stands for the matrix transpose of $\boldsymbol x$. Time-index $t=1,2,\dots, T$.

- $\boldsymbol \Omega$ is the initial mixing matrix, a shorthand for $\boldsymbol \Omega_0$; 
- $\boldsymbol{\mathcal E}$ a time-varying factor; 
- $t=1,2,\dots, T$ time-index;
- $l=1,2,3,\dots$ selected lag for time;
- Let $\boldsymbol \Lambda_{t,l} = \mathbb E( \boldsymbol z'_t \boldsymbol z'_{t+l})$ is the covariance matrix of the source signals (due to and zero-mean). Further, because of stationarity, $\boldsymbol\Lambda_l = \boldsymbol\Lambda_{t,l}$ for all $t$.
- Let $\boldsymbol R_l = \boldsymbol\Omega\ \boldsymbol\Lambda_l  \ \boldsymbol\Omega'$ a shorthand notation;


## Covariance matrix

$$\begin{aligned} 
\mathbb E( \boldsymbol x'_t \boldsymbol x_{t+l} )
&= \mathbb E \bigg( (1+t \boldsymbol{\mathcal E})\ \boldsymbol\Omega\ \boldsymbol z'_t\ \boldsymbol z_{t+l}\ \boldsymbol\Omega'\ (1+ (t+l) \boldsymbol{\mathcal E})'  \bigg) (p\times p)
\\ &= (1+t \boldsymbol{\mathcal E})\ \boldsymbol\Omega\ \mathbb E(\boldsymbol z'_t\ \boldsymbol z_{t+l})\ \boldsymbol\Omega'\ (1+ (t+l) \boldsymbol{\mathcal E})' &(1)
\\ &=(1+t \boldsymbol{\mathcal E})\ \boldsymbol\Omega\ \boldsymbol\Lambda_l  \ \boldsymbol\Omega'\ (1+ (t +l) \boldsymbol{\mathcal E})' &(2)
\\ &=(1+t \boldsymbol{\mathcal E})\ \boldsymbol R_l  \ (1+ (t +l) \boldsymbol{\mathcal E})' &(3)
\\ &= \boldsymbol R_l + t( \underline{\boldsymbol{\mathcal E}\, \boldsymbol R_l + \boldsymbol R_l \, \boldsymbol{\mathcal E}'}) + t^2(\underline{ \boldsymbol{\mathcal E}\, \boldsymbol R_l \, \boldsymbol{\mathcal E}'}) + tl ( \underline{\boldsymbol{\mathcal E}\, \boldsymbol R_l \, \boldsymbol{\mathcal E}'}) + l(\underline{ \boldsymbol R_l \, \boldsymbol{\mathcal E}'})
\end{aligned}$$

Above (1) is because of parameter-independence; (2) and (3) are defined notations. Furhter, observe that $\boldsymbol\Lambda_l$ and $\boldsymbol R_l$ are symmetric positive semi-definite matrices (followed by autocovariance and $A=U'U \Leftrightarrow A \succeq 0$).

Given a pre-defined lag $l$, $\begin{cases} \boldsymbol x_1' \boldsymbol x_{1+l} = \boldsymbol R_l + 1  (\underline{\boldsymbol{\mathcal E}\, \boldsymbol R_l + \boldsymbol R_l \, \boldsymbol{\mathcal E}'}) + (1^2 + l) \underline{ \boldsymbol{\mathcal E}\, \boldsymbol R_l \, \boldsymbol{\mathcal E}'} + l \underline{\boldsymbol R_l \, \boldsymbol{\mathcal E}'} \\ \boldsymbol x_2' \boldsymbol x_{2+l} = \boldsymbol R_l + 2  (\underline{\boldsymbol{\mathcal E}\, \boldsymbol R_l + \boldsymbol R_l \, \boldsymbol{\mathcal E}'}) + (2^2 + 2l) \underline{ \boldsymbol{\mathcal E}\, \boldsymbol R_l \, \boldsymbol{\mathcal E}'} + l \underline{\boldsymbol R_l \, \boldsymbol{\mathcal E}'} \\ \vdots \\ \boldsymbol x_{T-l}' \boldsymbol x_{T} = \boldsymbol R_l + (T-l)  (\underline{\boldsymbol{\mathcal E}\, \boldsymbol R_l + \boldsymbol R_l \, \boldsymbol{\mathcal E}'}) + ((T-l)^2 + (T-l)l) \underline{ \boldsymbol{\mathcal E}\, \boldsymbol R_l \, \boldsymbol{\mathcal E}'} + l \underline{\boldsymbol R_l \, \boldsymbol{\mathcal E}'} \end{cases}$

Consider element-wise equivalence, for $i,j=1,2,\dots, p$, the above equivalent to

$$
\underbrace{\begin{bmatrix} \boldsymbol x_1' \boldsymbol x_{1+l}\ [i,j] \\ \boldsymbol x_2' \boldsymbol x_{2+l}\ [i,j] \\ \vdots \\ \boldsymbol x_{T-l}' \boldsymbol x_{T}\  [i,j]\end{bmatrix}}_{:= \mathbf y}
= \underbrace {\begin{bmatrix} 1 & 1 & 1^2+l  \\  1 & 2 & 2^2+2l  \\ \vdots &\vdots &\vdots  \\  1 & T-l & (T-l)^2+(T-l)l \end{bmatrix}} _ {:= \mathbf H}  
\underbrace {\begin{bmatrix} \boldsymbol R_l \ [i,j] \\ ({\boldsymbol{\mathcal E}\, \boldsymbol R_l + \boldsymbol R_l \, \boldsymbol{\mathcal E}'})\ [i,j]\\ ({ \boldsymbol{\mathcal E}\, \boldsymbol R_l \, \boldsymbol{\mathcal E}'}) \ [i,j] \end{bmatrix}} _ {:= \boldsymbol \beta}
$$

$l$ is comparably small so that the last item would be ignorable; Alternatively, the following form would be more accurate.

$$
\underbrace{\begin{bmatrix} \boldsymbol x_1' \boldsymbol x_{1+l}\ [i,j] \\ \boldsymbol x_2' \boldsymbol x_{2+l}\ [i,j] \\ \vdots \\ \boldsymbol x_{T-l}' \boldsymbol x_{T}\  [i,j]\end{bmatrix}}_{:= \mathbf y}
= \underbrace {\begin{bmatrix} 1 & 1 & 1^2+l \\  1 & 2 & 2^2+2l \\ \vdots &\vdots &\vdots &\\  1 & T-l & (T-l)^2+(T-l)l \end{bmatrix}} _ {:= \mathbf H}  
\underbrace {\begin{bmatrix} (\boldsymbol R_l  +l \, {\boldsymbol R_l \, \boldsymbol{\mathcal E}'}) \ [i,j]\\ ({\boldsymbol{\mathcal E}\, \boldsymbol R_l + \boldsymbol R_l \, \boldsymbol{\mathcal E}'})\ [i,j]\\ ({ \boldsymbol{\mathcal E}\, \boldsymbol R_l \, \boldsymbol{\mathcal E}'}) \ [i,j] \end{bmatrix}} _ {:= \boldsymbol \beta}
$$

Therefore, for each $i,j = 1,2,\dots, p$ and $l\in L$, the linear model $\mathbf y = \mathbf H \boldsymbol \beta$ composed of $T-l$ observations are constructed.
